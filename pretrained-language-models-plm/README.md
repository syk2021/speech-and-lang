# Pretrained Language Models

Paper list from Yale's CPSC 488: AI Foundational Models, offered in Fall 2023, taught by Professor Arman Cohan.
https://yale-nlp.github.io/cpsc488/schedule/ &larr; Here is the course website!
p.s. I added some papers

## Word Embeddings, Tokenization
* GloVe: [GloVe - Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) (EMNLP Proceedings 2014)
* BPE: [BPE - Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Proceedings ACL 2016)
* SentencePiece: [SentencePiece: A Simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://aclanthology.org/D18-2012/) (Proceedings of EMNLP 2018 System Demonstrations)
* WordPiece: [Fast WordPiece Tokenization](https://aclanthology.org/2021.emnlp-main.160.pdf) (Proceedings EMNLP 2021)

## Transformers
* Attention is All You Need (NeurIPS 2017)

## Models
* ELMo: [Deep contextualized word representations](https://aclanthology.org/N18-1202/) (Proceedings NAACL-HLT 2018)
* ULMFit: [ULMFit, Universal Language Model Fine-tuning for Text Classification](https://aclanthology.org/P18-1031/) (Proceedings ACL 2018)
* BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf) (Proceedings NAACL-HLT 2019)
* ELECTRA: [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555) (ICLR 2020)
* T5: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (JMLR 2019)
* BART: [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://aclanthology.org/2020.acl-main.703/) (Proceedings ACL 2020)

## GPT-3, Few-Shot Learning, Prompting, In-context learning
* GPT-3: [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) (NeurIPS 2020)
* 
